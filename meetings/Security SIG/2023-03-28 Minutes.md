# BB TOC Security SIG 2023-03-28 Meeting Notes

## Runtime security research

### gVisor KVM notes 

gVisor does support KVM?

* https://github.com/google/gvisor/blob/05f88d6490de76aaa54c85f6e23a27fe54dbb312/g3doc/architecture_guide/platforms.md#kvm
* discourages nested virtualization for security. Also suggests performance is even slower than ptrace.
   * https://github.com/google/gvisor/blob/05f88d6490de76aaa54c85f6e23a27fe54dbb312/g3doc/user_guide/platforms.md

Not sure how this works. This is a non-userspace mode of gVisor because it uses KVM and real VMs. Does this effectively turn all syscalls into hypercalls and emulate them in gVisor?

### Some rootless k3s testing

https://docs.k3s.io/advanced#running-rootless-servers-experimental

Requires cgroups v2, plus some additional cgroup delegation config.
Most distros are on cgroups v2 now, but the additional non-default OS config is still required.

* https://rootlesscontaine.rs/getting-started/common/cgroup2/#checking-whether-cgroup-v2-is-already-enabled
  > The following distributions are known to use cgroup v2 by default:
  >
  > * Fedora (since 31)
  > * Arch Linux (since April 2021)
  > * openSUSE Tumbleweed (since c. 2021)
  > * Debian GNU/Linux (since 11)
  > * Ubuntu (since 21.10)
  > * RHEL and RHEL-like distributions (since 9)
* https://rootlesscontaine.rs/getting-started/common/cgroup2/#enabling-cpu-cpuset-and-io-delegation
  ```
  $ sudo mkdir -p /etc/systemd/system/user@.service.d
  $ cat <<EOF | sudo tee /etc/systemd/system/user@.service.d/delegate.conf
  [Service]
  Delegate=cpu cpuset io memory pids
  EOF
  $ sudo systemctl daemon-reload
  ```

* https://github.com/rootless-containers/usernetes

### KubeVirt 

kubevirt summit is this week, March 29-30 2023  
https://kubevirt.io/summit/

#### Unresolved(?) KubeVirt CVE-2023-26484

* https://github.com/kubevirt/kubevirt/security/advisories/GHSA-cp96-jpmq-xrr2
* https://github.com/kubevirt/kubevirt/issues/9109
* https://github.com/kubevirt/kubevirt/pull/9162

#### Kubernetes Cluster API Provider Kubevirt

https://github.com/kubernetes-sigs/cluster-api-provider-kubevirt

#### VMWare vm-operator? similar to kubevirt?

* https://github.com/vmware-tanzu/vm-operator
* https://vm-operator.readthedocs.io/en/stable/

## in-toto attestation v1.0.0 Released

https://github.com/in-toto/attestation/releases/tag/v1.0

## k8s native distros

Mentioned Karios and Talos, may be a good discussion topic during a future Security SIG meeting.

## Cilium Compatibility with BB NetworkPolicy

https://docs.cilium.io/en/latest/network/kubernetes/policy/#networkpolicy-state

> Known missing features for Kubernetes Network Policy:  
> ipBlock set with a pod IP https://github.com/cilium/cilium/issues/9209

`ipBlock` does not work with pod (or node?) IPs in Cilium. This makes it impossible to write a NetworkPolicy to allowlist traffic to the k8s api server (which is a (headless?) Service that points to the node IPs?).

BB currently provides two configurations that either:
1. deny only 169.254.169.254/32 (allow 0.0.0.0/0)
    * This is the default BB configuration
2. or allowlist a master node CIDR range

Both of these configurations specify an `ipBlock` which is incompatible with Cilium and actually results in blocking all cluster internal network traffic.

A Cilium specific `CiliumNetworkPolicy` alternative may work https://github.com/cilium/cilium/issues/20550. This is difficult to fit into BB because it is specific to one CNI.

Action: Update BB documentation?

* The deny 169.254.169.254 NetworkPolicy is incompatible with Cilium because it uses `ipBlock`
* This policy may be removed if you have an alternative method to block 169.254.169.254
* Or, the below CiliumNetworkPolicy may be used instead to allowlist communication to the k8s api server

```yaml
kind: CiliumNetworkPolicy
metadata:
  name: cilium-api-server-egress
  namespace: default
spec:
  endpointSelector:
    matchLabels: {} # all pods
  egress:
  - toEntities:
    - kube-apiserver
  - toPorts:
    - ports:
      - port: "6443"
        protocol: TCP
```

### Where to best set network policies in BB?

There are multiple places that these kinds of policies can be set. Should BB consider all of these and pick one? Or use multiple for defense in depth?

* NetworkPolicy
   * This seems the most CNCF-native default way to do things. But has some limitations vs more powerful tools like Istio. And apparently some CNIs have missing features
* CNI specific policy (e.g. CiliumNetworkPolicy)
   * This seems like a poor fit for BB because it's too specific to the users's k8s distro and CNI
* Istio
   * Historically, there's lots of ways to bypass the Istio sidecar. This may change with ambient mesh though, which may turn AuthorizationPolicies into a decent L4 firewall?
   * Some services still run without istio injection and would bypass policy
* Runtime security (e.g. Twistlock or Neuvector)
   * A runtime security tool is required for BB core and the DevSecOps Reference architecture, can we assume all users have a runtime security tool in place?
   * Do these provide similar network security policy capabilities?
   * Would BB need to provide policies for both tools?
